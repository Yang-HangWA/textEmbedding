
### 主要思想
- [from](https://kexue.fm/archives/8295)
- 我们真的在乎模版是不是“自然语言”构成的吗？

- 并不是。本质上来说，我们并不关心模版长什么样，我们只需要知道模版由哪些token组成，该插入到哪里，插入后能不能完成我们的下游任务，输出的候选空间是什么。模版是不是自然语言组成的，对我们根本没影响，“自然语言”的要求，只是为了更好地实现“一致性”，但不是必须的。
- 这里的[u1]～[u6]，代表BERT词表里边的[unused1]～[unused6]，也就是用几个从未见过的token来构成模板，这里的token数目是一个超参数，放在前面还是后面也可以调整。接着，为了让“模版”发挥作用，我们用标注数据来求出这个模板。 

```
这时候，根据标注数据量的多少，我们又分两种情况讨论。

第一种，标注数据比较少。这种情况下，我们固定整个模型的权重，只优化[unused1]～[unused6]这几个token的Embedding，
换句话说，其实我们就是要学6个新的Embedding，使得它起到了模版的作用。这样一来，因为模型权重几乎都被固定住了，
训练起来很快，而且因为要学习的参数很少，因此哪怕标注样本很少，也能把模版学出来，不容易过拟合。

第二种，标注数据很充足。这时候如果还按照第一种的方案来，就会出现欠拟合的情况，因为只有6个token的可优化参数实在
是太少了。因此，我们可以放开所有权重微调，原论文在SuperGLUE上的实验就是这样做的。读者可能会想：这样跟直接加个
全连接微调有什么区别？原论文的结果是这样做效果更好，可能还是因为跟预训练任务更一致了吧。
```
